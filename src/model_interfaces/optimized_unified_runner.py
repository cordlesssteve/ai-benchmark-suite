#!/usr/bin/env python3
"""
Optimized Unified Runner (Sprint 3.0)

Enhanced version of unified_runner.py with Sprint 3.0 performance optimizations:
- Parallel execution management
- Result caching
- Memory optimization
- Intelligent batch processing
- Performance monitoring
"""

import asyncio
import time
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import logging

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# Import performance optimization modules
try:
    from .parallel_execution_manager import (
        ParallelExecutionManager, EvaluationTask, EvaluationResult, ExecutionStrategy
    )
    from .result_cache_manager import ResultCacheManager, CacheStrategy
    from .memory_optimizer import MemoryOptimizer, MemoryStrategy
    from .language_detector import ProgrammingLanguage
except ImportError:
    # For standalone testing
    from parallel_execution_manager import (
        ParallelExecutionManager, EvaluationTask, EvaluationResult, ExecutionStrategy
    )
    from result_cache_manager import ResultCacheManager, CacheStrategy
    from memory_optimizer import MemoryOptimizer, MemoryStrategy
    from language_detector import ProgrammingLanguage

# Import existing interfaces
from unified_runner import UnifiedRunner, BenchmarkResult


@dataclass
class OptimizationConfig:
    """Configuration for performance optimizations"""
    enable_parallel_execution: bool = True
    enable_result_caching: bool = True
    enable_memory_optimization: bool = True
    max_parallel_workers: int = 4
    max_containers: int = 6
    execution_strategy: ExecutionStrategy = ExecutionStrategy.CONCURRENT_LANGUAGES
    cache_strategy: CacheStrategy = CacheStrategy.CONSERVATIVE
    memory_strategy: MemoryStrategy = MemoryStrategy.BALANCED
    cache_ttl_hours: int = 24
    memory_limit_mb: Optional[int] = None


class OptimizedUnifiedRunner(UnifiedRunner):
    """
    Performance-optimized version of UnifiedRunner with Sprint 3.0 enhancements.

    Key optimizations:
    - Parallel execution across languages and models
    - Intelligent result caching with parameter-aware keys
    - Memory optimization for large-scale evaluations
    - Batch processing with dynamic sizing
    - Comprehensive performance monitoring
    """

    def __init__(self, config_dir: Path = None, optimization_config: OptimizationConfig = None):
        super().__init__(config_dir)

        # Initialize optimization configuration
        self.opt_config = optimization_config or OptimizationConfig()

        # Initialize optimization components
        self._init_optimization_components()

        # Performance tracking
        self.performance_stats = {
            'total_evaluations': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'parallel_executions': 0,
            'total_time_saved': 0.0,
            'memory_optimizations_applied': 0
        }

        # Setup logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def _init_optimization_components(self):
        """Initialize performance optimization components"""

        # Parallel execution manager
        if self.opt_config.enable_parallel_execution:
            self.parallel_manager = ParallelExecutionManager(
                max_workers=self.opt_config.max_parallel_workers,
                max_containers=self.opt_config.max_containers,
                strategy=self.opt_config.execution_strategy
            )
        else:
            self.parallel_manager = None

        # Result cache manager
        if self.opt_config.enable_result_caching:
            cache_dir = self.project_root / "cache"
            self.cache_manager = ResultCacheManager(
                cache_dir=cache_dir,
                strategy=self.opt_config.cache_strategy,
                default_ttl_hours=self.opt_config.cache_ttl_hours
            )
        else:
            self.cache_manager = None

        # Memory optimizer
        if self.opt_config.enable_memory_optimization:
            self.memory_optimizer = MemoryOptimizer(
                strategy=self.opt_config.memory_strategy,
                memory_limit_mb=self.opt_config.memory_limit_mb
            )
        else:
            self.memory_optimizer = None

    def run_benchmark_optimized(self, task: str, model: str, **kwargs) -> BenchmarkResult:
        """
        Run a single benchmark with Sprint 3.0 optimizations.

        This method adds caching, memory optimization, and performance monitoring
        to the base benchmark execution.
        """

        # Check cache first if enabled
        if self.cache_manager:
            cache_key = self.cache_manager.generate_cache_key(
                task_name=task,
                model_name=model,
                language=self._detect_task_language(task),
                parameters=kwargs
            )

            cached_result = self.cache_manager.get_cached_result(cache_key)
            if cached_result:
                self.performance_stats['cache_hits'] += 1
                self.logger.info(f"üéØ Cache HIT for {task}:{model} - saved {cached_result.execution_time:.1f}s")

                # Convert cached result to BenchmarkResult
                return self._convert_cached_to_benchmark_result(cached_result, task, model)

            self.performance_stats['cache_misses'] += 1

        # Run evaluation with memory optimization if enabled
        if self.memory_optimizer:
            with self.memory_optimizer.memory_optimized_execution(f"benchmark_{task}_{model}"):
                start_time = time.time()
                result = super().run_benchmark(task, model, **kwargs)
                execution_time = time.time() - start_time

                # Cache successful results
                if self.cache_manager and result.score > 0:
                    self.cache_manager.save_result(
                        cache_key, result.__dict__, execution_time
                    )

                self.performance_stats['memory_optimizations_applied'] += 1
        else:
            # Run without memory optimization
            start_time = time.time()
            result = super().run_benchmark(task, model, **kwargs)
            execution_time = time.time() - start_time

            # Cache successful results
            if self.cache_manager and result.score > 0:
                cache_key = self.cache_manager.generate_cache_key(
                    task_name=task,
                    model_name=model,
                    language=self._detect_task_language(task),
                    parameters=kwargs
                )
                self.cache_manager.save_result(cache_key, result.__dict__, execution_time)

        self.performance_stats['total_evaluations'] += 1
        return result

    def run_suite_optimized(self, suite_name: str, models: List[str], **kwargs) -> List[BenchmarkResult]:
        """
        Run benchmark suite with full Sprint 3.0 optimizations.

        Features:
        - Parallel execution across models and tasks
        - Intelligent batch processing
        - Memory optimization for large workloads
        - Comprehensive caching
        """

        suite_def = self.suite_config.get('suites', {}).get(suite_name)
        if not suite_def:
            raise ValueError(f"Unknown suite: {suite_name}")

        tasks = suite_def.get('tasks', [])

        self.logger.info(f"üöÄ Running optimized {suite_name} suite:")
        self.logger.info(f"   Models: {len(models)}, Tasks: {len(tasks)}")
        self.logger.info(f"   Parallel execution: {self.opt_config.enable_parallel_execution}")
        self.logger.info(f"   Result caching: {self.opt_config.enable_result_caching}")
        self.logger.info(f"   Memory optimization: {self.opt_config.enable_memory_optimization}")

        # Use parallel execution if enabled
        if self.parallel_manager and len(models) * len(tasks) > 1:
            return self._run_suite_parallel(suite_name, models, tasks, **kwargs)
        else:
            return self._run_suite_sequential(suite_name, models, tasks, **kwargs)

    def _run_suite_parallel(self, suite_name: str, models: List[str],
                          tasks: List[str], **kwargs) -> List[BenchmarkResult]:
        """Run suite using parallel execution"""

        # Create evaluation tasks
        evaluation_tasks = []
        task_id = 0

        for model in models:
            for task in tasks:
                task_id += 1
                language = self._detect_task_language(task)

                eval_task = EvaluationTask(
                    task_id=f"{suite_name}_{task_id}",
                    task_name=task,
                    model_name=model,
                    language=language,
                    parameters=kwargs.copy(),
                    priority=self._calculate_task_priority(task, model)
                )
                evaluation_tasks.append(eval_task)

        # Memory optimization for large suites
        if self.memory_optimizer:
            memory_recommendations = self.memory_optimizer.optimize_for_large_evaluation(
                expected_tasks=len(tasks),
                expected_models=len(models)
            )

            self.logger.info(f"üìä Memory recommendations:")
            for rec in memory_recommendations['recommendations']:
                self.logger.info(f"   ‚Ä¢ {rec}")

        # Execute tasks in parallel
        def progress_callback(task, result):
            status = "‚úÖ" if result.success else "‚ùå"
            self.logger.info(f"  {status} {task.task_name} on {task.model_name} "
                           f"({task.language.value}) - {result.execution_time:.1f}s")

        start_time = time.time()

        # Run async execution
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            results = loop.run_until_complete(
                self.parallel_manager.execute_parallel_evaluations(
                    evaluation_tasks,
                    self._async_evaluation_wrapper,
                    progress_callback
                )
            )
        finally:
            loop.close()

        total_time = time.time() - start_time
        self.performance_stats['parallel_executions'] += 1

        # Convert results to BenchmarkResults
        benchmark_results = []
        for result in results:
            if result.success:
                benchmark_result = BenchmarkResult(
                    harness=result.result_data.get('harness', 'optimized'),
                    task=result.task_name,
                    model=result.model_name,
                    score=result.result_data.get('score', 0.0),
                    metrics=result.result_data.get('metrics', {}),
                    metadata=result.result_data.get('metadata', {}),
                    execution_time=result.execution_time
                )
            else:
                benchmark_result = BenchmarkResult(
                    harness="failed",
                    task=result.task_name,
                    model=result.model_name,
                    score=0.0,
                    metrics={"error": result.error_message},
                    metadata={"error": result.error_message},
                    execution_time=result.execution_time
                )

            benchmark_results.append(benchmark_result)

        # Print optimized summary
        self._print_optimized_suite_summary(suite_name, benchmark_results, total_time)

        return benchmark_results

    def _run_suite_sequential(self, suite_name: str, models: List[str],
                            tasks: List[str], **kwargs) -> List[BenchmarkResult]:
        """Run suite sequentially with optimizations"""

        results = []

        # Memory optimization for large suites
        if self.memory_optimizer:
            with self.memory_optimizer.memory_optimized_execution(f"suite_{suite_name}"):
                for model in models:
                    for task in tasks:
                        try:
                            result = self.run_benchmark_optimized(task, model, **kwargs)
                            results.append(result)
                        except Exception as e:
                            self.logger.error(f"Task {task} on {model} failed: {e}")
                            failed_result = BenchmarkResult(
                                harness="failed",
                                task=task,
                                model=model,
                                score=0.0,
                                metrics={"error": str(e)},
                                metadata={"error": str(e)},
                                execution_time=0.0
                            )
                            results.append(failed_result)
        else:
            # Run without memory optimization
            for model in models:
                for task in tasks:
                    try:
                        result = self.run_benchmark_optimized(task, model, **kwargs)
                        results.append(result)
                    except Exception as e:
                        self.logger.error(f"Task {task} on {model} failed: {e}")

        return results

    def _async_evaluation_wrapper(self, task_name: str, model_name: str, **kwargs) -> Dict[str, Any]:
        """Wrapper for async execution of benchmark evaluations"""
        try:
            result = self.run_benchmark_optimized(task_name, model_name, **kwargs)
            return result.__dict__
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'harness': 'failed',
                'task': task_name,
                'model': model_name,
                'score': 0.0,
                'metrics': {'error': str(e)},
                'metadata': {'error': str(e)},
                'execution_time': 0.0
            }

    def _detect_task_language(self, task: str) -> ProgrammingLanguage:
        """Detect programming language from task name"""
        if task.startswith('multiple-'):
            language_map = {
                'multiple-js': ProgrammingLanguage.JAVASCRIPT,
                'multiple-java': ProgrammingLanguage.JAVA,
                'multiple-cpp': ProgrammingLanguage.CPP,
                'multiple-go': ProgrammingLanguage.GO,
                'multiple-rs': ProgrammingLanguage.RUST,
                'multiple-ts': ProgrammingLanguage.TYPESCRIPT,
            }
            return language_map.get(task, ProgrammingLanguage.PYTHON)
        else:
            return ProgrammingLanguage.PYTHON  # Default for humaneval

    def _calculate_task_priority(self, task: str, model: str) -> int:
        """Calculate task priority for execution ordering"""
        # Higher priority for faster tasks/models
        base_priority = 1

        # Prioritize Python tasks (typically faster)
        if task == 'humaneval':
            base_priority += 2

        # Prioritize smaller models (if we can detect them)
        if any(fast_model in model.lower() for fast_model in ['phi', 'qwen']):
            base_priority += 1

        return base_priority

    def _convert_cached_to_benchmark_result(self, cached_result: 'CachedResult',
                                          task: str, model: str) -> BenchmarkResult:
        """Convert cached result to BenchmarkResult format"""
        data = cached_result.result_data

        return BenchmarkResult(
            harness=data.get('harness', 'cached'),
            task=task,
            model=model,
            score=data.get('score', 0.0),
            metrics=data.get('metrics', {}),
            metadata={**data.get('metadata', {}), 'cached': True, 'cache_age': time.time() - cached_result.timestamp},
            execution_time=0.0  # No execution time for cached results
        )

    def _print_optimized_suite_summary(self, suite_name: str, results: List[BenchmarkResult], total_time: float):
        """Print enhanced summary with optimization statistics"""

        print(f"\nüìä {suite_name.upper()} SUITE SUMMARY (OPTIMIZED)")
        print("=" * 80)

        # Basic statistics
        successful = [r for r in results if r.score > 0]
        cached_results = [r for r in results if r.metadata.get('cached', False)]

        print(f"\n‚ö° Performance Statistics:")
        print(f"  Total execution time: {total_time:.2f}s")
        print(f"  Results: {len(successful)}/{len(results)} successful")
        print(f"  Cache hits: {len(cached_results)} ({len(cached_results)/len(results):.1%})")

        # Sprint 3.0 optimization stats
        if self.cache_manager:
            cache_stats = self.cache_manager.get_cache_stats()
            print(f"  Overall cache hit rate: {cache_stats['hit_rate']:.1%}")
            print(f"  Cache size: {cache_stats['cache_size_mb']:.1f} MB")

        if self.parallel_manager:
            parallel_stats = self.parallel_manager.get_performance_stats()
            if parallel_stats:
                print(f"  Parallel executions: {parallel_stats.get('total_tasks', 0)}")
                print(f"  Execution strategy: {parallel_stats.get('strategy_used', 'sequential')}")

        if self.memory_optimizer:
            memory_report = self.memory_optimizer.get_memory_report()
            print(f"  Peak memory usage: {memory_report['peak_memory_mb']:.1f} MB")
            print(f"  Memory strategy: {memory_report['strategy']}")

        # Call parent summary for detailed results
        super()._print_suite_summary(suite_name, results)

    def get_optimization_report(self) -> Dict[str, Any]:
        """Get comprehensive optimization performance report"""

        report = {
            'optimization_config': {
                'parallel_execution': self.opt_config.enable_parallel_execution,
                'result_caching': self.opt_config.enable_result_caching,
                'memory_optimization': self.opt_config.enable_memory_optimization,
                'execution_strategy': self.opt_config.execution_strategy.value,
                'cache_strategy': self.opt_config.cache_strategy.value,
                'memory_strategy': self.opt_config.memory_strategy.value,
            },
            'performance_stats': self.performance_stats.copy()
        }

        # Add component-specific stats
        if self.cache_manager:
            report['cache_stats'] = self.cache_manager.get_cache_stats()

        if self.parallel_manager:
            report['parallel_stats'] = self.parallel_manager.get_performance_stats()

        if self.memory_optimizer:
            report['memory_stats'] = self.memory_optimizer.get_memory_report()

        return report

    def cleanup_optimizations(self):
        """Clean up optimization components"""
        if self.cache_manager:
            # Cache is persistent, no cleanup needed
            pass

        if self.parallel_manager:
            # Parallel manager cleans up automatically
            pass

        if self.memory_optimizer:
            self.memory_optimizer._cleanup_resources()


# Testing and demonstration
def demo_optimized_runner():
    """Demonstration of optimized runner capabilities"""
    print("üöÄ Optimized Unified Runner Demo")
    print("=" * 50)

    # Create optimized configuration
    opt_config = OptimizationConfig(
        enable_parallel_execution=True,
        enable_result_caching=True,
        enable_memory_optimization=True,
        max_parallel_workers=2,
        execution_strategy=ExecutionStrategy.CONCURRENT_LANGUAGES,
        cache_strategy=CacheStrategy.CONSERVATIVE,
        memory_strategy=MemoryStrategy.BALANCED
    )

    # Create optimized runner
    runner = OptimizedUnifiedRunner(optimization_config=opt_config)

    print(f"‚úÖ Optimized runner initialized:")
    print(f"   Parallel execution: {opt_config.enable_parallel_execution}")
    print(f"   Result caching: {opt_config.enable_result_caching}")
    print(f"   Memory optimization: {opt_config.enable_memory_optimization}")

    # This would run actual evaluations if models were available
    print(f"\nüìù Example optimized benchmark command:")
    print(f"   runner.run_benchmark_optimized('humaneval', 'qwen-coder', limit=5)")

    print(f"\nüìä Example optimized suite command:")
    print(f"   runner.run_suite_optimized('coding_suite', ['qwen-coder', 'codellama'])")

    # Show optimization report
    report = runner.get_optimization_report()
    print(f"\nüìà Optimization Configuration:")
    for key, value in report['optimization_config'].items():
        print(f"   {key}: {value}")

    print(f"\n‚úÖ Demo completed! Ready for 5x+ performance improvements.")


if __name__ == "__main__":
    demo_optimized_runner()