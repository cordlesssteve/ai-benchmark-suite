# Task to Harness Mappings
# Defines which evaluation harness to use for each task

task_mappings:
  # Contamination-resistant tasks (RECOMMENDED)
  livecodebench_v6: livecodebench
  livecodebench_v6_full: livecodebench
  livecodebench_recent: livecodebench
  bigcodebench: bigcodebench
  bigcodebench_full: bigcodebench
  bigcodebench_sample: bigcodebench
  swebench_live_latest: swebench_live
  swebench_live_month: swebench_live

  # BigCode tasks (code generation - LEGACY)
  humaneval: bigcode
  mbpp: bigcode
  apps: bigcode
  ds1000: bigcode
  humanevalpack: bigcode
  multiple-py: bigcode
  multiple-js: bigcode
  multiple-java: bigcode

  # LM-Eval tasks (language understanding)
  hellaswag: lm_eval
  arc_easy: lm_eval
  arc_challenge: lm_eval
  winogrande: lm_eval
  mathqa: lm_eval
  gsm8k: lm_eval
  truthfulqa: lm_eval
  mmlu: lm_eval
  piqa: lm_eval
  openbookqa: lm_eval

# Harness-specific configurations
harness_configs:
  bigcode:
    executable: "harnesses/bigcode-evaluation-harness/venv/bin/python"
    main_script: "main.py"
    working_directory: "harnesses/bigcode-evaluation-harness"
    default_args:
      allow_code_execution: true
      save_generations: true
      precision: "fp16"
      batch_size: 1

  lm_eval:
    executable: "python"
    main_script: "-m lm_eval"
    working_directory: "harnesses/lm-evaluation-harness"
    default_args:
      model: "hf"
      batch_size: "auto"
      device: "auto"

  livecodebench:
    executable: "harnesses/livecodebench/.venv/bin/python"
    main_script: "-m lcb_runner.runner.main"
    working_directory: "harnesses/livecodebench"
    default_args:
      release_version: "release_v6"
      scenario: "codegeneration"
      n_samples: 10
      temperature: 0.2

  bigcodebench:
    executable: "harnesses/bigcodebench/.venv/bin/python"
    main_script: "-m bigcodebench.generate"
    working_directory: "harnesses/bigcodebench"
    default_args:
      subset: "full"
      split: "complete"
      backend: "vllm"

  swebench_live:
    executable: "harnesses/swebench-live/.venv/bin/python"
    main_script: "-m swebench.harness.run_evaluation"
    working_directory: "harnesses/swebench-live"
    default_args:
      split: "test"
      max_workers: 4
      timeout: 300

# Model type detection for harness selection
model_patterns:
  bigcode_models:
    - "codeparrot"
    - "starcoder"
    - "santacoder"
    - "incoder"
    - "codegen"
    - "code-"
    - "coder"

  general_models:
    - "gpt"
    - "claude"
    - "llama"
    - "mistral"
    - "phi"
    - "qwen"

# Custom harness definitions
custom_harnesses:
  # Placeholder for future custom evaluation frameworks
  experimental:
    description: "Experimental evaluation framework"
    tasks: []