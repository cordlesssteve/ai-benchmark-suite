# Benchmarking Methodology: The Prompting Problem

## üéØ **The Core Challenge**

You've identified one of the **most significant issues** in modern AI benchmarking: **prompt sensitivity**. Different models respond to different prompt formats, making fair comparison extremely difficult.

## üìö **How Research Handles This**

### **1. Standardized Prompt Templates**
Major benchmarks define **exact prompt formats** that all models must use:

**HumanEval (Original)**:
```python
# Exact prompt format mandated by benchmark
def function_name(args):
    """Docstring"""
    # Model must complete from here
```

**MBPP (Original)**:
```python
# Test-driven format
"""
Write a function to accomplish X.
assert function_name(input) == expected_output
"""
```

### **2. Model-Type Specific Adaptations**

Research papers typically test multiple prompt formats and report all results:

**Base Models (like Codex):**
- Direct completion: `def add(a, b):\n    return`

**Instruction Models (like ChatGPT):**
- Conversational: `"Complete this Python function: def add(a, b):\n    return"`

**Chat Models:**
```json
{
  "messages": [
    {"role": "user", "content": "Complete this function: def add(a, b):"}
  ]
}
```

### **3. Pass@K with Multiple Sampling**

The research community addresses prompt sensitivity through **statistical approaches**:

- **Pass@1**: Single attempt success rate  
- **Pass@5**: Best of 5 attempts
- **Pass@10**: Best of 10 attempts
- **Pass@100**: Best of 100 attempts

This reduces the impact of prompt formatting by allowing multiple tries.

### **4. Few-Shot vs Zero-Shot Evaluation**

**Zero-Shot** (what we're doing):
```python
def has_close_elements(numbers, threshold):
    """Check if any two numbers are closer than threshold"""
    # Complete here
```

**Few-Shot** (with examples):
```python
# Example 1:
def add(a, b):
    """Add two numbers"""
    return a + b

# Example 2: 
def multiply(a, b):
    """Multiply two numbers"""
    return a * b

# Now complete:
def has_close_elements(numbers, threshold):
    """Check if any two numbers are closer than threshold"""
    # Complete here
```

## üî¨ **Research Best Practices**

### **1. Multiple Prompt Templates**
Leading papers test 3-5 different prompt formats:

```python
# Template A: Direct completion
prompt_a = function_signature + "\n    "

# Template B: Instruction format  
prompt_b = f"Complete this function:\n{function_signature}\n    "

# Template C: Conversational
prompt_c = f"Please implement this Python function:\n{function_signature}"

# Template D: Few-shot with examples
prompt_d = f"{examples}\n\nNow complete:\n{function_signature}"

# Template E: Chain-of-thought
prompt_e = f"Let's solve this step by step:\n{function_signature}"
```

### **2. Statistical Reporting**
Papers report results across all formats:

| Model | Template A | Template B | Template C | Template D | Template E | Average |
|-------|------------|------------|------------|------------|------------|---------|
| GPT-4 | 67.1% | 73.2% | 71.8% | 76.4% | 74.9% | 72.7% |
| Claude| 61.3% | 69.7% | 68.2% | 72.1% | 70.8% | 68.4% |

### **3. Model-Agnostic Evaluation**
Some frameworks try to be "model-agnostic":

**OpenAI Evals Framework:**
- Defines standard interfaces
- Model-specific adapters handle formatting
- Results normalized across model types

**EleutherAI Harness:**
- Template system for different model architectures
- Automatic prompt adaptation based on model type

### **4. Confidence Intervals**
Report statistical significance:
```
Model A: 67.1% ¬± 2.3% (95% CI)
Model B: 69.7% ¬± 1.9% (95% CI)
```

## üè≠ **Industry Approaches**

### **1. OpenAI's Approach (Codex Paper)**
- Used **direct completion** for base models
- Used **instruction format** for InstructGPT
- Reported both results separately
- Acknowledged this as a limitation

### **2. Google's PaLM/Bard Papers**
- Tested **5 different prompt formats**
- Used **bootstrap sampling** for confidence intervals
- Reported "best prompt" and "average across prompts"

### **3. Anthropic's Constitutional AI**
- **Extensive prompt engineering** across model families
- **Human evaluation** of prompt fairness
- **Red team testing** for prompt gaming

### **4. Meta's CodeLlama**
- **Standardized evaluation harness**
- **Model-specific optimization** acknowledged but controlled
- **Ablation studies** on prompt sensitivity

## ‚öñÔ∏è **The Fairness Problem**

### **The Dilemma:**
- **Too standardized**: May unfairly penalize models trained differently
- **Too optimized**: May give unfair advantage to models with better prompting

### **Current Research Solutions:**

**1. Ensemble Prompting**
```python
def evaluate_model(model, problem):
    prompts = [template_a(problem), template_b(problem), template_c(problem)]
    scores = [evaluate(model.generate(prompt)) for prompt in prompts]
    return {
        'best_score': max(scores),
        'avg_score': mean(scores),
        'worst_score': min(scores)
    }
```

**2. Prompt Optimization Budgets**
- Allow each model N attempts at prompt engineering
- Report results with same "optimization budget"

**3. Meta-Learning Approaches**
- Learn optimal prompts automatically
- Use same prompt optimization across all models

## üõ†Ô∏è **Practical Recommendations for Our Suite**

### **1. Multi-Template Evaluation**
```python
def benchmark_with_multiple_prompts(model, problem):
    templates = {
        'direct': lambda p: p.signature + "\n    ",
        'instruction': lambda p: f"Complete this function:\n{p.signature}",
        'conversational': lambda p: f"Please implement:\n{p.signature}",
        'few_shot': lambda p: f"{examples}\n\nComplete:\n{p.signature}"
    }
    
    results = {}
    for name, template in templates.items():
        prompt = template(problem)
        result = model.generate(prompt)
        results[name] = evaluate(result, problem.test)
    
    return results
```

### **2. Statistical Aggregation**
```python
# Report multiple metrics
{
    'best_prompt_score': max(template_scores),
    'average_score': mean(template_scores),  
    'worst_prompt_score': min(template_scores),
    'std_dev': std(template_scores),
    'prompt_sensitivity': max(scores) - min(scores)
}
```

### **3. Model-Type Detection**
```python
def detect_model_type(model_name):
    if 'instruct' in model_name.lower():
        return 'instruction'
    elif 'chat' in model_name.lower():
        return 'conversational'
    elif 'base' in model_name.lower():
        return 'completion'
    else:
        return 'unknown'

def get_optimal_prompt_template(model_type, problem):
    templates = {
        'completion': lambda p: p.signature + "\n    ",
        'instruction': lambda p: f"Complete this function:\n{p.signature}",
        'conversational': lambda p: f"Please implement this function:\n{p.signature}"
    }
    return templates[model_type](problem)
```

## üìä **Research Examples**

### **CodeT5 Paper (2021)**
- Tested **4 prompt formats**
- Reported results for each format
- Used **Pass@1, Pass@5, Pass@10**
- Acknowledged "prompt engineering as future work"

### **AlphaCode Paper (2022)**  
- Used **competition-specific formatting**
- **No prompt optimization** - used exact contest format
- Argued this was "more fair" but acknowledged limitations

### **CodeGen Paper (2022)**
- **Extensive prompt sensitivity analysis**
- Tested **8 different formats**
- Found up to **15% variance** between prompts
- Recommended "ensemble evaluation"

## üéØ **Bottom Line**

**The field hasn't fully solved this yet.** Current best practices:

1. **Acknowledge the problem explicitly**
2. **Test multiple prompt formats** when possible  
3. **Report prompt sensitivity metrics**
4. **Use statistical significance testing**
5. **Be transparent about methodology**

For your Ollama models, we could implement a multi-template approach to get more representative results across different prompting strategies.

**The key insight**: Perfect fairness may be impossible, but **transparency and multiple measurement approaches** help build confidence in results.